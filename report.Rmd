---
title: "Artificially Intelligent Wine Tasting"
subtitle:  "A report on the performance and utility of an \"AI Sommelier\" "
author: "Student Number: 16035918, 16044460, 17107203"
header-includes:
   - \usepackage{amsmath, amsfonts, nicefrac, bm, fancyhdr}
   - \pagestyle{fancy}
   - \fancyhf{}
   - \rhead{16035918, 16044460, 17107203}
   - \rfoot{Page \thepage}
   - \usepackage{booktabs}
   - \usepackage{longtable}
   - \usepackage{array}
   - \usepackage{multirow}
   - \usepackage[table]{xcolor}
   - \usepackage{wrapfig}
   - \usepackage{float}
   - \usepackage{colortbl}
   - \usepackage{pdflscape}
   - \usepackage{tabu}
   - \usepackage{threeparttable}
   - \usepackage[normalem]{ulem}
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  bookdown::pdf_document2:
    number_sections: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, #cache = TRUE, cache.lazy = FALSE,
                      fig.pos = "H")
```

```{r load_pkgs, message=FALSE, warning=FALSE}

# use pacman for package management
if(!require("pacman")) {
    install.packages("pacman")
}

# load the tidyverse, joyplots (now in ggridges) for plots,
# magrittr for pipes, summarytools for descriptives
pacman::p_load(
  tidyverse, ggridges, magrittr, summarytools,
  GGally, mclust, feather, reticulate, purrr, here, stringr,
  kableExtra, broom, knitr)
```

```{r load_data, message=FALSE}

# read in the csv files
wine <- 
  bind_rows(
    # read in white, encode as 0
    read_delim('./input/winequality-white.csv', delim=';', guess_max=10000) %>%
      mutate(color=0L),
    # read in red, encode as 1
    read_delim('./input/winequality-red.csv', delim=';', guess_max=10000) %>%
      mutate(color=1L)
  )
```


```{r helper-functions}
recode_color_ <- function(x) dplyr::recode(x, `1` = 'Red', `0` = 'White')
recode_color <- function(x){
  # used to avoid factors, which can be capricious
  
  switch(
    is.data.frame(x) + 1,
    # if not a dataframe, directly recode
    recode_color_(x),
    # if a dataframe, overwrite color column (presumed to exist)
    dplyr::mutate(x, color=recode_color_(color))
  )
}
```

# Executive Summary
## Project Goals

[TO FOLLOW]


## Findings

[TO FOLLOW]


## Conclusions

[TO FOLLOW]


## Limitations

[TO FOLLOW]

\newpage

# Introduction

## Overview

This research project investigates the performance and utility of an "AI sommelier" - Delicious AI's flagship, eponymous, artificial intelligence application for the automatic identification and rating of high quality wines. For Delicious AI, the "AI sommelier" is likely to be the first of many artificial intelligence applications focusing on the food and drinks market. Understanding the "AI sommelier's" performance and usability is vital for Delicious AI's long-term vision of creating artificial intelligences with an appreciation for quality food. In order to develop an AI which can create high quality novel cuisine, it is important to establish whether an AI can identify quality in the first place.

As part of the initial scoping out of this project, we have been employed by Delicious AI to investigate the possibility of using machine learning techniques to automatically evaluate a range of high quality wines in a reliable, accurate, efficient, and scalable manner. We have further been tasked with exploring whether it will be possible for an algorithm to provide the recipe for a wine better than any seen before. In this report we summarise the results of our investigations and provide our recommendations for your "AI sommelier" product. We trust that it will provide you with useful information on the performance and utility of an "AI sommelier", and additionally inform the extent to which you allocate analytical resources towards developing your food and drink AIs.

## Research questions

*Discuss, before doing any analyses, which of the questions, numbered 1 to 4 in the scenario outline on p.4-5, you can answer, or to which extent you can come close to answering the questions, using only the data provided. Explicitly state the scientific, empirically quantifiable questions that you are going to address. In the final report, this should be summarized as a separate (optimally, the second) section, which does not contain any analyses.*

The first question we consider is whether we can use the dataset to create an AI with super-human performance in wine-tasting. It is important to note that we are training Delicious AI to predict how a wine would be rated by human wine sommeliers, so its predictions should align with subjective human judgements of wine quality, not a "super-human" ground truth (whatever that may be). However, Delicious AI will still have other qualities which make it super-human in wine-tasting tasks.  First, Delicious AI will have an inhuman ability to be consistent in its ratings of wines. While a human sommelier may occasionally evaluate the same bottle of wine differently depending on mood, context and other factors, Delicious AI will reliably give the same rating to a given bottle of wine, with a known margin of error. Second, once a data-gathering system is in place at a winery, Delicious AI will be super-human in its ability to rate wines at scale. An individual sommelier would need days or weeks to systematically taste and evaluate every batch of wine produced by a winery. Further, each wine would have to be evaluated by multiple sommeliers in order to calculate a mean rating that averages out the potential biases and differences between individual sommeliers. Meanwhile, Delicious AI alone can evaluate all of the batches of wine in a matter of minutes, incorporating the subjective tastes of a variety of human sommeliers. Perhaps most significantly, Delicious AI can periodically evaluate the wines at different stages of the production process, predicting the future quality of a wine before it even exists. One of the main focuses of our investigation will be developing an early-prediction algorithm for wine quality.

Second, we would like to determine which components of wine constitute a wine a good wine. In order to empirically analyze this question, we define a "good" wine as a wine which is rated highly (above 5) by the sommeliers in our dataset. In theory, we could also identify wines considered "good" by individuals of a specific demographic (eg, young consumers in eastern Europe), but this would require additional data, so we will instead focus on finding the components of wine that are associated with high ratings from sommeliers. The simplest way to scientifically investigate this question is to analyze correlation plots between wine quality and individual variables from the wine dataset. Similarly, we can look at the coefficients in a linear regression model to see how much an increase in the value of a particular variable will increase the predicted rating, conditioned on the other variables. We can additionally use a two-sample t-test to determine whether the mean of a variable is statistically different among good wines and bad wines, comparing the distributions of the variable when the data is partitioned between the two groups. Each of these methods will provide new insight into which components are associated with higher ratings. There may be additional components of wine outside the scope of our dataset which serve as important indicators of quality, but we would need additional data to investigate this possibility.

Next, we would like to assess whether we can create a "perfect" wine whose quality exceeds that which we have seen in our data. In order to conceive a grade 10 wine, Delicious AI would have to extrapolate beyond the labels and combinations of predictors seen in the training data, so it is difficult to guarantee that the model would be valid for these new ranges of the variables. We have no reason to believe that the assumptions made by the models will hold for a novel combination of chemicals, nor do we make any claims that such a combination is physically possible or safe for human consumption. [We must also assume that the dataset contains all the relevant variables necessary to produce a high-grade wine.] Despite the lack of performance guarantees, we can still use Delicious AI for creative inspiration, identifying which novel combinations of variables Delicious AI would expect to produce a perfect wine. In this way, Delicious AI can suggest new combinations of features that wine producers may not have otherwise considered.

Finally, we consider a question raised by a philosopher of ethics - is the human perception of wine quality so subjective that there is no empirically verifiable correlate of good and bad wine? This is a perfectly reasonable question to ask, considering that new sommeliers are trained to match the opinions of existing sommeliers, so human biases could easily perpetuate for generations. This consideration aside, in theory, the blind-tasting of the wines should have mitigated the influence of factors exogenous to the wines themselves. It would be helpful to know the variance of ratings for each wine and how consistently human sommeliers rate the same wine. For example, would a sommelier rate a wine differently if it were presented in a different bottle? We would need to conduct an experiment to answer these questions. However, we can still use the dataset for insight into whether wines rated highly tend to have a different set of features than wines rated poorly. We can again look at the variable distributions and use a two-sample t-test to determine whether the mean of a predictor is different among good and bad wines. We can also see how accurately Delicious AI can predict wine ratings from the features alone. A high prediction accuracy will at least confirm that the ratings among sommeliers are not random and correspond to empirically verifiable differences in wine components. Of course, Delicious AI is only trained to approximate *expert* opinion, which is not necessarily an objective measure of wine quality and may differ from individual preferences or the preferences of the general wine-drinking public. But this is still valuable because expert opinion tends to drive market prices, and we can always train Delicious AI on different data in order to predict individual or demographically relevant ratings. Similarly, with additional data we Delicious AI could predict external metrics such as quantity sold or awards recieved.
        
The specific, scientific, empirically quantifiable questions we propose to answer in this project are:

1. Supposing that certain variables are detectable early in the wine-making process, is it possible to train a model on these variables alone that is sufficiently predictive of the sommelier ratings?
2. Which features are most strongly associated with high-quality ratings?
3. Is it possible to simulate a (reasonable) dataset of wines, upon some subset of which our "best" learner estimates a rating of 10?
4. Can we produce a model that is predictive of the expert ratings? 
5. Do the raw data support relationships between objective qualities and the ratings? 
6. Can an unsupervised method determine any clusters that bear some relationship to ratings (avg. silhouette width/adjusted rand index)? [Probably cut]


# Methods and data

In this section we describe the data and modelling approaches we undertook in this study. We also outline all key decisions made in our work.

## Methods
The main purpose of our investigation was to explore the feasibility of an "AI sommelier". In order to do this, we conducted a series of predictive benchmarking experiments to compare the performance and utility of various machine learning algorithms. 

We used each algorithm to determine:

1. whether, and how well wine quality can be predicted from chemical composition and colour;
2. whether wine colour adds predictive power above chemical composition and vice versa, in 1.

Our predictive benchmarking experiments were first carried out with wine quality treated as a continuous variable (univariate regression) and then repeated with wine quality treated as a categorical variable (deterministic classification). The nature of the wine quality scale is such that both approaches were deemed reasonable.

### Algorithms
The following five algorithms were analysed in all experiments:

- an intercept-only dummy regressor model (DR)
- a linear regression model (LR)
- a non-linear support vector machine (SVM),
- an ensemble of (at least 10) trees (RF)
- a neural network with two or more middle layers (MLP)

We used the intercept-only dummy regressor (mean) model as a "best guess" performance baseline against which the other algorithms could be compared. The linear/logistic regression model was chosen because it is a simple, fast, and easy to understand model that would allow us to understand relationships between the quality of a wine and its characteristics. The SVM has previously been identified as the best performing model for classifying wine by quality (Cortez et. al. 2009). Like the linear regression model, Random Forests are relatively easy to understand and quick to implement. Additionally, they are able to deal with a minimally pre-processed input data, can perform automatic feature selection, and provide information on the importance of model features. Finally, neural networks are also able to create and combine new features independently, and perform well on classification tasks. Although it is harder to interpret the relationship between features and classification decisions, neural networks have the advantage of being able to work with unlabelled data, so that Delicious AI could develop its own taste; to carry out multitask learning, which is useful when your company expands to other products; and to learn embedded representations of data, such as the taste sensor data that may be available in the future.

### Optimisation
In order to achieve the best performance from each algorithm for each task, we optimised model hyper-parameters using randomised search with 3-fold cross-validation. In the randomised search method, a fixed number of parameter settings is sampled from specified distributions, and then optimised by cross-validated search over parameter settings. Since the randomised search method does not test every single parameter in the search space, it is considerably faster than a full grid search. Using cross-validation allowed us to make the most of our training data, whilst also lowering the risk of overfitting our model to a specific train/validation split. Hyper-parameters were optimised separately for the regression task (to minimise the mean squared error) and for the classification task (to maximise accuracy). The following hyper-parameters were optimised or pre-selected for each learner and each task (where relevant), with all other parameters set to their defaults:

* Linear/logistic regression: 
    + Penalty: L1 or L2
    + C: sampled from an exponential distribution with $\lambda = 0.1$
* SVM:
    + C: sampled from an exponential distribution with $\lambda = 0.1$
    + Gamma:  sampled from an exponential distribution with $\lambda = 10$
    + Kernel: Radial Basis Function (pre-selected)
* Random Forest:
    + Number of trees: between 10 and 100
    + Number of features: between 1 and 11
    + Maximum tree depth: None (pre-selected)
* Multilayer Perceptron:
    + Activation functions: ReLU or Sigmoid
    + Hidden layer size/number of neurons in a hidden layer: 32, 64 or 128
    + Learning rate: sampled from a normal distribution with $\mu = 0.001$,  $\sigma = 0.0002$
    + Maximum iterations: 500 (pre-selected)

### Performance metrics

Two performance metrics were used to compare the predictive performance of our learners in the univariate regression task - mean squared error and mean absolute error. In addition to these two metrics, an accuracy score was also evaluated for the classification task. 

### Model validation

The standard protocols for carrying out a supervised learning benchmarking experiment were followed in this study. Of the 6,497 wines in the dataset, a random 20% sample (1,299 wines) was held out as a separate test set. The same test set was used for all benchmarking experiments to evaluate the performance and generalisability of our different models on previously unseen data. The test set was not used for any model training or hyper-parameter optimisation purposes. The remaining 80% sample (5,198 wines) was used as a training set upon which each model could be learned and hyper-parameters could be optimised.

The learners were all evaluated on the same test set for both the regression and classification tasks using the previously outlined performance metrics. The mean and 95% bootstrapped confidence intervals for the each performance metric on the test set were calculated for each algorithm and each task. Bootstrapped resamples with replacement were taken from the test set in order to do this. Each resample was the size of the test set (1,299 wines) and the resampling process was repeated for 1,000 iterations. The results of this process can be seen in section [XX].

### Additional methodological considerations

In line with the project specification provided by Delicious AI, all wines were retained in all benchmarking experiments i.e. outliers were not removed. We used the full range of wine quality values present in the dataset i.e. the target variable was not grouped into broader categories. Both red and white wines were analysed together so that models learned from a single dataset, rather than a separate model by colour. For consistency across learners, we did not specify any feature interactions, as different learners determine relevant model features in their own ways. All variable transformations were carried on both training and test sets. The relative importance of different features was evaluated using the RF regression model.

[More here…]

## Data

We used the Wine Quality Dataset for this project. This systematic data on 1599 red and 4898 white vinho verde quality wines from Portugal was acquired specifically to help the AI sommelier acquire its taste for wine. Wine quality was measured using the median sensory preference for a wine from up to three sensory assessors. These sensory preferences were decided following blind sensory assessment on a subjective scale of 0 (disgusting) to 10 (excellent). In the absence of a truly objective measure of wine quality, we used this subjective scale in order to train the AI sommelier towards an "expert's taste" for wine. We used the extensive physiochemical data on each wine as features in our models. Laboratory analysis of the wine provided the following 11 variables (in addition the wine quality measure):


1. fixed acidity = mass concentration of tartaric acid (g=dm^3)
2. volatile acidity = mass concentration of acetic acid (g=dm^$)
3. mass concentration of citric acid (g=dm^3)
4. residual sugar mass concentration (g=dm^3)
5. mass concentration of sodium chloride (g=dm^3)
6. mass concentration of free sulfur dioxide (mg=dm^3)
7. mass concentration of sulfur dioxide total (mg=dm^3)
8. density (g=cm^3)
9. pH value
10. mass concentration of potassium sulphate (mg=dm^3)
11. alcohol content (vol%)

Summary statistics and further descriptive, exploratory analysis can be seen in the next section.

# Exploratory analysis

First, we confirm that there are no missing entries in the dataset. We also notice that there are many more white wines (4898) than red wines (1599). Next, for each variable, we plot a summary of the range, mean, median, and first and third quartiles, shown below. A few variables in particular stand out. Citric acid, for example, has a minimum value of zero. Further investigation shows that there are 151 zero entries for citric acid. This indicates that it is an optional component in wine-making. In addition, we see that different variables take on quite different ranges of values. For instance, while "volatile acidity" ranges from 0.08 to 1.58, "free sulfur dioxide" ranges from 1 to 289. This suggests that we should try models in which we normalize the variables, although it would be easier to interpret unscaled models. We also notice that the ratings range from 3 to 9, indicating that no wines were "perfect" enough to achieve a 10 or awful enough to be rated 1 or 2.

<5-number summaries>
```{r summary-stats}
q025 <- function(x) quantile(x, 0.25)
q075 <- function(x) quantile(x, 0.75)

wine_summary <-
  wine %>%
  
  # summary statistics by color
  # perhaps a little cleaner to look overall?
  # group_by(color) %>%
  summarize_all(
    funs(min, q025, median,  mean, q075, max, sd)
  ) %>%
  
  # transform data
  # gather(... = -color) %>%
  gather %>%
  separate(key, into = c('Variable', 'statistic'), sep = '_') %>%
  spread(key = statistic, value = value)

# order variables with highest variation (after rescaling)
variable_var <-
  wine %>%
  select(#-color,
         -quality) %>%
  # min-max scale the data
  mutate_all(funs(. / (max(.) - min(.)))) %>%
  # take variances
  summarize_all(var) %>%
  # reshape and sort
  gather %>%
  arrange(-value)

# make beautiful
wine_summary %>%
  # recode_color %>%
  select(
    Variable,
    # Color=color,
    Min = min,
    `1st Quantile` = q025,
    Median = median,
    Mean = mean,
    `3rd Quantile` = q075,
    Max = max,
    `S.D.`=sd
  ) %>%
  arrange(Variable) %>%
  kable(format = "latex", 
        booktabs = T,
        digits = 3,
        caption = "Summary of Wine Data") %>% 
  kableExtra::row_spec(0, bold = TRUE) %>% 
  kableExtra::kable_styling(latex_options = "hold_position")
```

<Correlations>
```{r cor-heatmap, fig.height=6, fig.width=6}
wine_cor <-
  wine %>%
  select(-quality, -color) %>%
  mutate_all(log1p) %>%
  # get correlations
  cor

wine_cor[upper.tri(wine_cor, diag = TRUE)] <- NA
  
wine_cor %>%
  reshape2::melt() %>%
  # plot
  ggplot(aes(x=Var1, y=Var2, fill=value)) +
  geom_tile() +
  scale_fill_gradient2(name = 'Pearson Corr.',
                       limit=c(-1, 1)) + 
  # geom_text(aes(label=round(value, 2))) +
  theme(axis.title.x = element_blank(),
        axis.title.y = element_blank(),
        axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1)) +
  coord_fixed()
  
```

We see that quite a few variables are skewed right or have large outliers, as indicated by a higher mean than median and relatively high maximum. These include chlorides, residual sugar, and sulphates. Residual sugar in particular has a median of 3 but max of 65.8. Upon looking at a table of values we see that this is due to just one outlier and the second highest value is 31.6. Histograms representing the distributions of these three variables can be found below.

<distributions of chlorides, residual sugar, and sulphates>
```{r histograms}
# plot histograms (unscaled)
wine %>%
  select(chlorides, `residual sugar`, sulphates) %>%
  #mutate_at(vars(-alcohol, -pH, -quality), log) %>%
  gather(key='Variable', value='Value') %>%
  ggplot(aes(x = Value)) +
  geom_histogram(bins = 50) +
  facet_wrap(~Variable, scale='free') +
  ggtitle('Variable Histograms')
```

To correct for the skew, we consider log-transforming the predictors. The box-and-whisker plots (below) show how the log-transformations even out the distributions of individual variables. However, we decide to keep the variables untransformed in order to retain the interpretability of our models. [Also: cite something stating how concentration is usually log-scaled? https://www.ncbi.nlm.nih.gov/pubmed/7595977 is for blood, but could be useful. https://stats.stackexchange.com/a/48465/80111 has another good intuition]

<a few box-and-whisker plots of variables before and after taking log>

```{r variable_transformations}
# Do we need to convert the concentrations that are mg/dm3 compared to those that are g/dm3?

# convert all concentration variables to the log scale
origwine <- wine

concvars <- c(
  "fixed acidity",
  "volatile acidity",
  "citric acid",
  "residual sugar",
  "chlorides",
  "free sulfur dioxide",
  "total sulfur dioxide",
  "sulphates"
)

wine <- 
    origwine %>% 
    mutate_at(.vars = vars(concvars),
              .funs = log1p) %>% 
    # scale the data (can turn off later)
    mutate_at(vars(-color, -quality), funs(scale(.) %>% as.numeric))
```

A series of matrix plots do not suggest that there are any non-linear relationships in the data. However, we do see a positive linear relationship between "free sulfur dioxide" and "total sulfur dioxide." The correlation coefficient for the two variables is 0.72. It is not surprising that the variables are related, since total sulfur dioxide is defined as the sum of free and bound sulfur dioxide. There is also slight collinearity between residual sugar and density, which has a correlation coefficient of 0.55.

<random matrix plot>
<pairs plot: free sulfur dioxide vs total sulfur dioxide>
<pairs plot: residual sugar vs density>
```{r pairs-plot, message=FALSE}
# pairs plot of the n variables with the greatest variance
n <- 5
p <-  
  wine %>%
  recode_color %>%
  ggpairs(
    mapping=aes(color=color, alpha=0.4),
    columns=c('free sulfur dioxide',
              'total sulfur dioxide',
              'residual sugar',
              'density')
    # if we want n variables with greatest variance, change to
    # variable_var$key[1:n]
  )
for(i in 1:p$nrow) {
  for(j in 1:p$ncol){
    p[i,j] <- p[i,j] + 
      scale_color_manual(values = c("Red" = "#E31A1C", "White" = "#33A02C")) +
      scale_fill_manual(values = c("Red" = "#E31A1C", "White" = "#33A02C"))
  }
}

p
```

Next, we consider how wine color affects the variables. There are 4898 white wines and 1599 red wines. White wines tend to have a higher rating. 66% of white wines are rated above 5 and five white wines are rated 9, while only 53% of red wines are rated above 5 and none of them receive a rating of 9. To visualize the differences, we consider the distribution of each variable, partitioned by the discrete integer-valued ratings and wine color (below). Some of the density plots are based on a small number of samples - only 10 red wines and 20 white wines are rated 3, and only 18 red wines are rated 8. We therefore focus our analysis on the distributions corresponding to ratings between 4 and 7, which both have over 50 examples for each color. 

<Meena plots>
[NB: unscaled data!]
```{r plots, message=FALSE, warning=FALSE}
# ridge plot function
ridgeplot <- function(winevar) {
    origwine %>%
    recode_color %>%
    ggplot(aes(y = quality)) +
    geom_density_ridges(aes_q(x = as.name(winevar), 
                              fill = paste(origwine$quality, origwine$color)), 
                        alpha = .8, 
                        color = "white", 
                        rel_min_height = 0.01) +
    labs(x = Hmisc::capitalize(gsub(x = winevar,
                                    pattern = "\\.",
                                    replacement = " ")),
         y = "Wine quality") +
    scale_y_discrete(expand = c(0.01, 0)) +
    scale_x_continuous(expand = c(0.01, 0)) +
    scale_fill_cyclical(breaks = c("3 red", "3 white"),
                        labels = c(`3 red` = "Red", `3 white` = "White"),
                        values = c("3 red" = "#E31A1C",
                                   "3 white" = "#33A02C",
                                   "4 red" = "#FB9A99",
                                   "4 white" = "#B2DF8A",
                                   "5 red" = "#E31A1C",
                                   "5 white" = "#33A02C",
                                   "6 red" = "#FB9A99",
                                   "6 white" = "#B2DF8A",
                                   "7 red" = "#E31A1C",
                                   "7 white" = "#33A02C",
                                   "8 red" = "#FB9A99",
                                   "8 white" = "#B2DF8A",
                                   "9 white" = "#33A02C"),
                        name = "Wine colour", 
                        guide = "legend") +
    theme_ridges(grid = FALSE, font_size = 11)
}

# make ridgeplots
ridgeplotlist <- lapply(names(origwine)[1:11], ridgeplot)
ridgeplots_grid <- ggpubr::ggarrange(plotlist = ridgeplotlist, ncol = 2, nrow = 2)
ridgeplots_grid
```

We see from the plots that the color of the wine does indeed interact with the predictors. The plots reveal that red wines tend to have higher fixed acidity, volatile acidity, chlorides, pH and sulphates. Alternatively, white wines have higher citric acid, total sulfur dioxide and a distinct right-skew in the distribution of residual sugar. This confirms a natural intuition that it is more common for white wines to be sweet than red wines and that red wines tend to have more sulphates. We also notice an interesting trend with citric acid. While low-quality red wines tend to have less citric acid than low-quality white wines, the trend appears to reverse for high-quality wines. Red wine also has a wider range of citric acid values. However, it is hard to be certain that these particular trends are significant, since the number of samples is low for wines with especially high and low ratings. 

These graphs also indicate that certain variables are distributed differently among high- and low-quality wines. The white wines seem to have more stable distributions across ratings than the red wines, mostly bunched up into a small range with just a few outliers. For the red wines, very high volatile acidity seems to be an indication that the wine is of poor quality. We also observe that highly rated red wines tend to have more sulphates. Finally, the strongest indicator of wine quality seems to be alcohol, where quality tends to increase with alcohol percentage for both red and white wines alike.


# Results of predictive benchmarking experiment

```{r save-data, eval=FALSE, include=FALSE}
# save the data in binary format for python interoperability
write_feather(wine, './intermediate/wine_logged_scaled.feather')
origwine %>%
  mutate_at(.vars = vars(concvars),
            .funs = log1p) %>%
  write_feather('./intermediate/wine_logged_unscaled.feather')
```

```{python load-data, engine.path='C:/Miniconda3/envs/py36/python.exe', eval=FALSE}
data_fpath = './intermediate/wine_logged_unscaled.feather'
  
## Data Collection 
# all functions below maintain same train-test split
# all data
x_train, x_test, y_train, y_test = gen_data(data_fpath)
# color only
x_train_col, x_test_col, y_train_col, y_test_col = gen_data(
  data_fpath, features_to_drop=[c for c in x_train.columns if c != 'color'])
# chemicals only
x_train_chm, x_test_chm, y_train_chm, y_test_chm = gen_data(
  data_fpath, features_to_drop=['color'])
```

```{python train-models, engine.path='C:/Miniconda3/envs/py36/python.exe', eval=FALSE}
# train on all data
train('unscaled', x_train, y_train)

# train on colors only
train('unscaled-color', x_train_col, y_train_col)

# train on chemicals, no color
train('unscaled-chemical', x_train_chm, y_train_chm)
```

```{python eval-models, engine.path='C:/Miniconda3/envs/py36/python.exe', eval=FALSE}
## Evaluation
results_all = evaluate_models('unscaled', x_test, y_test)
results_col = evaluate_models('unscaled-color', x_test_col, y_test_col)
results_chm = evaluate_models('unscaled-chemical', x_test_chm, y_test_chm)

#save 
results_all.to_csv('./output/results_unscaled_all.csv', index=False)
results_col.to_csv('./output/results_color_all.csv', index=False)
results_chm.to_csv('./output/results_chemical_all.csv', index=False)

## Model-specific results
regressors, classifiers = load_models('unscaled')
# get rf importances
rf_importances = rf_feature_importances(regressors['rf'], x_train.columns)
rf_importances.to_csv('./output/rf_importances_all.csv', index=False)
```

```{r make_benchmark_outputs, include=FALSE}

# Function to make formatted benchmark results table
benchmarktable <- function(file, task, measure) {

df <- read.csv(file = here("output", file))
capvars <- paste(str_split(str_replace(file, ".csv", ""), "_", simplify = TRUE)[3:2], collapse = " ")

df %>% 
  filter(type == task) %>% 
  filter(metric %in% measure) %>% 
  select(-type) %>%
  mutate(metric = case_when(
    metric == "f1_score" ~ "Accuracy",
    TRUE ~ str_to_upper(metric)
    )) %>% 
  mutate(model = case_when(
    model %in% c("dummy","linear") ~ str_to_title(model),
    TRUE ~ str_to_upper(model)
    )) %>% 
  rename_at(., vars(starts_with("m")), str_to_title) %>% 
  rename_at(., vars(ends_with("b")), str_to_upper) %>% 
  kable(digits = 4,
        booktabs = T,
        format = "latex",
        caption = paste("Benchmarking", "results -", ifelse(task == "reg", "regression", "classification"), "task with", capvars, "variables")) %>% 
  kableExtra::row_spec(0, bold = TRUE) %>% 
  kableExtra::kable_styling(latex_options = "hold_position")

}

# Function to make formatted benchmarking plots

benchmarkplot <- function(file, task, measure) {
  
  df <- read.csv(file = here("output", file))
  capvars <- paste(str_split(str_replace(file, ".csv", ""), "_", simplify = TRUE)[3:2], collapse = " ")
  fmeasure <- ifelse(measure == "f1_score", "Accuracy", str_to_upper(measure))
  
  df %>% 
    filter(type == task) %>% 
    filter(metric %in% measure) %>% 
    select(-type) %>%
    mutate(metric = case_when(
      metric == "f1_score" ~ str_replace(str_to_title(metric), "_", " "),
      TRUE ~ str_to_upper(metric)
    )) %>% 
    mutate(model = case_when(
      model %in% c("dummy","linear") ~ str_to_title(model),
      TRUE ~ str_to_upper(model)
    )) %>% 
    rename_at(., vars(starts_with("m")), str_to_title) %>% 
    rename_at(., vars(ends_with("b")), str_to_upper) %>% 
    ggplot(aes(Mean, Model, color = Model)) +
    geom_point() +
    geom_errorbarh(aes(xmin = LB, xmax = UB)) +
    theme(legend.position = "none") +
    xlab(paste("Mean", fmeasure)) +
    theme(text = element_text(size = 12)) 
  # +
  #   ggtitle(paste("Benchmarking", fmeasure, "results -", ifelse(task == "reg", "regression", "classification"), "task with", capvars, "variables")) +
  #   theme(plot.title = element_text(size = 10))
}

benchmarkplot <- function(file, task, measure) {
  
  df <- read.csv(file = here("output", file))
  capvars <- paste(str_split(str_replace(file, ".csv", ""), "_", simplify = TRUE)[3:2], collapse = " ")
  fmeasure <- ifelse(measure == "f1_score", "Accuracy", str_to_upper(measure))
  
  df %>% 
    filter(type == task) %>% 
    filter(metric %in% measure) %>% 
    select(-type) %>%
    mutate(metric = case_when(
      metric == "f1_score" ~ str_replace(str_to_title(metric), "_", " "),
      TRUE ~ str_to_upper(metric)
    )) %>% 
    mutate(model = case_when(
      model %in% c("dummy","linear") ~ str_to_title(model),
      TRUE ~ str_to_upper(model)
    )) %>% 
    rename_at(., vars(starts_with("m")), str_to_title) %>% 
    rename_at(., vars(ends_with("b")), str_to_upper) %>% 
    
    ggplot(aes(Model, Mean, fill = Model)) +
    geom_bar(stat='identity') +
    scale_fill_brewer(type = 'qual', palette = 4) +
    geom_errorbar(aes(ymin = LB, ymax = UB),
                   position = position_dodge(), width=0.3) +
    theme(legend.position = "none") +
    xlab(paste("Mean", fmeasure)) +
    theme(text = element_text(size = 12),
          panel.grid.major = element_blank(),
          panel.grid.minor = element_blank(),
          panel.background = element_blank(),
          axis.line = element_line(color = 'black'))
}


# a function to get the stat I want
get_stat <- function(file, task, learner, measure, digits = 3) {
  
  df <- read.csv(file = here::here("output", file))

  # Pipes not working in inline R code - even though they were before...!?!?!
  # df %>% 
  #   filter(type == task) %>% 
  #   filter(metric == measure) %>% 
  #   filter(model == learner) %>% 
  #   select(mean) %>% 
  #   round(digits) %>% 
  #   as_vector()
  
  y <- subset(x = df, subset = type == task & metric == measure & model == learner, select = "mean")
  y <- round(y, digits)
  return(y)
}

# a function to get the average stat across learners
get_avg_stat <- function(file, task, learners = c("rf", "linear", "mlp", "svm")) {
   
  y <- sapply(X = learners, function(x) get_stat(file = "results_chemical_all.csv", task = "reg", learner = x, measure = "mse", digits = 3))
  y <- mean(unlist(y))
  y <- round(y, 3)
  return(y)
  
}

round(mean(unlist(sapply(X = c("rf", "linear", "mlp", "svm"), function(x) get_stat(file = "results_chemical_all.csv", task = "reg", learner = x, measure = "mse", digits = 3)))),3)

# Get the parameters into lists for pmap

files <- c("results_unscaled_all.csv", "results_chemical_all.csv", "results_color_all.csv")
tasks <- c("reg", "clf")
measures <- c("mse", "mae", "f1_score")

benchdata <- list(files = files, tasks = tasks, measures = measures)

# Make the benchmarking tables
benchtablelist <- 
  benchdata %>%
    purrr::cross_df() %>% 
    filter(!(tasks == "reg" & measures == "f1_score")) %>% 
    pmap(., ~ benchmarktable(..1, ..2, ..3))

# Make the benchmarking plots
benchplotlist <- 
  benchdata %>%
  purrr::cross_df() %>% 
  filter(!(tasks == "reg" & measures == "f1_score")) %>% 
  pmap(., ~ benchmarkplot(..1, ..2, ..3))

```

# Results

## Results of the predictive benchmarking experiment

### Regression task

The results of the predictive benchmarking experiment for the regression task can be seen in Table \@ref(tab:regtask-results-tab). The random forest algorithm is the best performer on both metrics under consideration for the regression task, achieving a mean squared error of `r get_stat(file = "results_unscaled_all.csv", task = "reg", learner = "rf", measure = "mse")` and a mean absolute error of `r get_stat(file = "results_unscaled_all.csv", task = "reg", learner = "rf", measure = "mae")`. For comparison, the dummy regressor achieved a mean squared error of `r get_stat(file = "results_unscaled_all.csv", task = "reg", learner = "dummy", measure = "mse")` and a mean absolute error of `r get_stat(file = "results_unscaled_all.csv", task = "reg", learner = "dummy", measure = "mae")`. On the MSE metric, the SVM is the next best performer, followed by the linear model, the MLP, and finally the dummy regressor.

```{r regtask-results-tab}

benchmarktable(file = "results_unscaled_all.csv", 
               task = "reg", 
               measure = c("mse", "mae"))
```

Figure \@ref(fig:regtask-results-fig) plots these results for each algorithm.

```{r regtask-results-fig, echo=FALSE, fig.cap="Benchmarking results - regression task with all variables", fig.show='hold', out.width='50%'}

benchmarkplot(file = "results_unscaled_all.csv", 
              task = "reg", 
              measure = "mse") 

benchmarkplot(file = "results_unscaled_all.csv", 
              task = "reg", 
              measure = "mae") 

```


The performance of the algorithms on the same task but with a reduced set of predictor variables can be seen in Figure \@ref(fig:regtask-results-fig2). The left-hand plot shows model performance (mean squared error) with wine colour as the only predictor. The right-hand plot shows results for physiochemical predictors only. Interestingly, all five algorithms have an almost identical, large, MSE (`r get_avg_stat(file = "results_color_all.csv", task = "reg")`) for the wine colour only model, suggesting that wine colour alone is not a strong predictor of quality. In contrast, replacing wine colour with the physiochemical characteristics of a wine leads to differential performance across algorithms, and an average MSE of `r get_avg_stat(file = "results_chemical_all.csv", task = "reg")` for all algorithms excluding the dummy regressor, which had an unchanged performance.

```{r regtask-results-fig2, fig.cap="Benchmarking results - regression task with wine colour only (left) and physiochemical variables only (right)", fig.show='hold', out.width='50%'}

benchmarkplot(file = "results_color_all.csv", 
              task = "reg", 
              measure = "mse") 

benchmarkplot(file = "results_chemical_all.csv", 
              task = "reg", 
              measure = "mse") 

```


### Classification task

The results of the predictive benchmarking experiment for the classification task can be seen in Table \@ref(tab:clftask-results-tab). As with the regression task, the random forest model was again the best performer, achieving classification accuracy of `r get_stat(file = "results_unscaled_all.csv", task = "clf", learner = "rf", measure = "f1_score")` and a mean absolute error `r get_stat(file = "results_unscaled_all.csv", task = "clf", learner = "rf", measure = "mae")`. For comparison, the dummy regressor achieved a classification accuracy of `r get_stat(file = "results_unscaled_all.csv", task = "clf", learner = "dummy", measure = "f1_score")` and a mean absolute error `r get_stat(file = "results_unscaled_all.csv", task = "clf", learner = "dummy", measure = "mae")`. The ranking of algorithms by classification accuracy was the same as for the regressio task, with the SVM following the RF, then the linear model, the MLP, and the dummy regressor in fifth position.

```{r clftask-results-tab}

benchmarktable(file = "results_unscaled_all.csv", 
               task = "clf", 
               measure = c("f1_score", "mae"))

```


Figure \@ref(fig:clftask-results-fig) plots these results for each algorithm.

```{r clftask-results-fig, fig.cap="Benchmarking results - classification task with all variables", fig.show='hold' , out.width='50%'}

benchmarkplot(file = "results_unscaled_all.csv", 
              task = "clf", 
              measure = "f1_score") 

benchmarkplot(file = "results_unscaled_all.csv", 
              task = "clf", 
              measure = "mae") 

```

The performance of the algorithms on the same task but with a reduced set of predictor variables can be seen in Figure \@ref(fig:clftask-results-fig2). The left-hand plot shows model accuracy with wine colour as the only predictor. The right-hand plot shows results for physiochemical predictors only. Interestingly, all algorithms except the dummy regressor achieve identical classification accuracy (`r get_avg_stat(file = "results_color_all.csv", task = "clf")`) when using wine colour as the only model feature. As with the regression task, replacing wine colour with the physiochemical characteristics of a wine leads to differential performance across algorithms, and an average MSE of `r get_avg_stat(file = "results_chemical_all.csv", task = "clf")` for all algorithms excluding the dummy regressor, which was again unchanged.

```{r clftask-results-fig2, fig.cap="Benchmarking results - classification task with wine colour only (left) and physiochemical variables only (right)", fig.show='hold', out.width='50%'}

benchmarkplot(file = "results_color_all.csv", 
              task = "clf", 
              measure = "f1_score") 

benchmarkplot(file = "results_chemical_all.csv", 
              task = "clf", 
              measure = "f1_score") 

```

## Important components of a "good" wine

A useful characteristic of the random forest algorithm is its ability to determine the relative salience of the various features utilised during training. 

```{r rf-impvar-results-tab}

rf_impvar <- read.csv(here("output", "rf_importances_all.csv"))

rf_impvar %>% 
  mutate(feature = as.character(feature)) %>% 
  mutate(feature = case_when(
    feature == "pH" ~ "pH",
    TRUE ~ Hmisc::capitalize(feature)
  )) %>% 
  kable(format = "latex", 
        booktabs = T,
        digits = 3, 
        col.names = c("Feature", "Mean", "SD"), 
        caption = "Relative feature importance in classifying wine quality") %>% 
  kableExtra::row_spec(0, bold = TRUE) %>% 
  kableExtra::kable_styling(latex_options = "hold_position")

```

```{r rf_impvar_get_stat}

# a function to get key stats from the RF importance table

rf_get_stat <- function(feat) {
  
  rf_impvar <- read.csv(here::here("output", "rf_importances_all.csv"))

  # Pipes don't seem to work in inline r code in bookdown::pdf_document2 style
  
  # rf_impvar %>% 
  #   filter(feature == feat) %>% 
  #   select(mean) %>% 
  #   round(3) %>% 
  #   as_vector()
  
  y <- subset(x = rf_impvar, subset = feature == feat, select = "mean")
  y <- round(y, 3)
  return(y)
}

```


Table \@ref(tab:rf-impvar-results-tab) shows the relative importance of each of the variables in the Wine Quality dataset with regards to its use in predicting wine quality. As can be seen, alcohol(vol%) is the most important feature, with a mean importance of `r rf_get_stat("alcohol")`, folowed by volatile acidity i.e. mass concentration of acetic acid (`r rf_get_stat("volatile acidity")`) and density (`r rf_get_stat("density")`). Fixed acidity (`r rf_get_stat("fixed acidity")`) and colour (`r rf_get_stat("color")`) are the least useful features, suggesting wines of both colours and across the range of mass concentrations of tartaric acid can be "good" wines (and "bad" wines).
 

## The search for the perfect wine

By creating mixtures of chemicals (and color) -- limited only to the ranges observed in the test data -- we attempt to simulate a wine that will achieve a perfect "10" rating by our models' estimates (reiterating that such a wine may be impossible to create in the physical world, is missing other key components, and is "perfect" only in the eyes of the trained models for which it is optimized).

We use a support vector machine (SVM) and linear regression model to produce two possible "super-wines", since they are both capable of predicting values beyond the range of data used in training. For the linear regression model, we maximize the values of each feature known to correspond to high quality wines, and minimize those associated with low-quality wines. For the SVM, we can randomly sample values for each predictor (within the range of the training data) and feed the artificial wine into the SVM, stopping when we achieve a rating above 10.

```{python superwine, engine.path='C:/Miniconda3/envs/py36/python.exe', eval=FALSE}
# Superwine creation
lin = regressors['linear'].best_estimator_
svm = regressors['svm'].best_estimator_

# Obtain bounds of training data
feature_bounds = [(x_test[f].min(), x_test[f].max()) for f in x_train]

# Simulate a wine using the linear regression coefficients
simwine_lin = np.array([b[c > 0] for b, c in zip(feature_bounds, lin.coef_)])
simwine_lin = simwine_lin.reshape(1, -1)

# Simulate a wine using by sampling until we get a wine "better than any one
# we have seen", based on the SVM
pred = 0
while pred < 10.0:
  simwine_svm = np.array([np.random.uniform(*b) for b in feature_bounds])
  simwine_svm = simwine_svm.reshape(1, -1)
  # restrict the last feature, the color, to be red or white
  simwine_svm[:, -1] = np.random.choice((0 , 1))
  pred = svm.predict(simwine_svm)

# Save the two simulations, noting their performance on the models
simwines = pd.DataFrame(
  {'linear': simwine_lin[0], 'svm': simwine_svm[0]}, index=x_train.columns)
simwines.to_csv('./output/simwine_data.csv')

simwine_preds = pd.DataFrame({
  name: model.predict(np.vstack([simwine_lin, simwine_svm]))
  for name, model in regressors.items()
}, index = ['linear', 'svm'])
simwine_preds.to_csv('./output/simwine_preds.csv')
```

Unfortunately, these "superwines" fail to achieve 10 ratings across all models, which is to be expected, since they were optimized for these particular models. However, they do indicate certain trends. As seen in Table \@ref(tab:simwine-preds-tab), they tend to have higher levels of free sulfur dioxade but lower levers of total sulfur dioxide than the average wine in the data, and have increased levels of chlorides and residual sugar.

```{r superwine-eval, message=FALSE, warning=FALSE}
simwine_preds <- read_csv('./output/simwine_preds.csv')
simwines <- read_csv('./output/simwine_data.csv')
```

```{r simwine-preds-tab}
simwines %>%
  mutate(avg = origwine %>% select(-quality) %>% summarize_all(mean) %>% t,
         
         linear = if_else(X1 %in% concvars, exp(linear), linear),
         svm = if_else(X1 %in% concvars, exp(svm), svm),
         
         linear = sprintf('%.2f', linear) %>% recode('1.00'='Red'),
         svm = sprintf('%.2f', svm) %>% recode('0.00'='White'),
         avg = sprintf('%.2f', avg) %>% recode('0.25'='25% Red')
         ) %>%
  select(X1, avg, linear, svm) %>%
  kable(format = "latex", 
        booktabs = T,
        digits = 3,
        align = 'r',
        col.names = c('Variable', 'Mean','Linear', 'SVM'),
        caption = "Simulated Wine Characteristics") %>% 
  kableExtra::row_spec(0, bold = TRUE) %>%
  kableExtra::kable_styling(latex_options = "hold_position")
```

```{r simwine-preds-fig1, fig.cap="Simulated Wine Performance by Model", fig.show='hold', out.width='75%'}
simwine_preds %>%
  gather(key='Model', value='Rating', -X1) %>%
  rename(`Optimized for` = X1) %>%
  mutate(
    Model = recode(
      Model,
      dummy = 'Dummy',
      linear = 'Linear', 
      svm = 'SVM', 
      rf = 'Random Forest', 
      mlp = 'Neural Net'
    ),
    `Optimized for` = recode(
      `Optimized for`,
      linear = 'Linear',
      svm = 'SVM'
    ),
    highlight = `Optimized for` == `Model`
  ) %>%
  ggplot() +
  geom_bar(aes(x = Model, y = Rating, fill = `Optimized for`),
           stat='identity', position = position_dodge()) +
  scale_fill_brewer(type='qual', palette = 1) +
  geom_bar(aes(x = Model, y = Rating, fill = `Optimized for`, color = highlight),
           alpha=0, stat='identity', position = position_dodge(), size=1.25) +
  scale_color_manual(values=c(NA, "black"), guide = FALSE) +
  theme(panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(),
        panel.background = element_blank(),
        axis.line = element_line(color = 'black'))

```

## A 'super-human' AI

Although our models are likely to rate wines with greater consistentcy than sommeliers -- and perhaps make more accurate predictions on objective metrics, like provenance -- without more information we are unable to determine with certainty if Delicious AI can outperform humans' predictive performance on any given metric. Lacking data on measurable human performance, we cannot speak to a strictly "superhuman" capacity.

However, our models are, on average, able to rate wines to within a close margin of sommelier rankings using only a limited subset of information (e.g., the models cannot assess the aroma directly). Moreover, some of these characteristics are present early in the winemaking process, perhaps well before an expert could make a judgement of quality (although here too we would need more information to make such a determination).

```{r superhuman-results, message=FALSE}
sh_rf_mse <- 
  read_csv('./output/results_superhuman_all.csv') %>%
  filter(type == 'reg', model == 'rf', metric == 'mse') %>%
  summarize_at(vars('mean', 'lb', 'ub'), round, 2)

all_rf_mse <- 
  read_csv('./output/results_unscaled_all.csv') %>%
  filter(type == 'reg', model == 'rf', metric == 'mse') %>%
  summarize_at(vars('mean', 'lb', 'ub'), round, 2)
```

Namely, a wine's color, fixed acidity, citric acid, residual sugar, and chlorides are all largely determined by grape characteristics rather than effects of the fermentation process. Limiting ourselves to these components alone, our best model acheives a mean squared error of `r sh_rf_mse$mean`, [`r sh_rf_mse$lb`, `r sh_rf_mse$ub`], which is worse than the best model trained on all variables (`r all_rf_mse$mean`, [`r all_rf_mse$lb`, `r all_rf_mse$ub`]), but nonetheless represents an improvement over the naive baseline, performing on par with the linear model that uses all data. Obtaining such an early estimate could help guide the manufacturing process, alerting winemakers to potential problems with a batch.

Although human winemakers indeed use similar variables to select harvest times, it is within the realm of possibility that Delicious AI could make judgements of resulting wine quality with "super-human" prescience and accuracy.

# Conclusions

## Key findings

## Recommendations

## Limitations

- Something about how the data was collected - we don't know the distribution of the experts' sensory assessments for each wine, or even how the judgements were carried out (were they truly blind taste tests etc.?)
- We lack knowledge of how different experts were from one another – if you had a sense of their distributions, you could re-fit the data on augmented, noise-added data

## Future research
Currently Delicious AI has been trained to match the wine experts' taste. It may make more commercial sense, however, to train an AI to match the taste of your company's customer base, or even the taste of the general public. This could be done by carrying out another wine tasting experiment with ordinary members of the public, perhaps following the protocols of a randomised control trial, to see how perceptions of wine quality differ (both between experts and ordinary people, and between the AIs). Alternatively, other measures of "success" could be used instead of subjective wine quality, such as volume sold, profit made, or wine prizes won. Delicious AI could be trained to predict these outcomes instead of, or in addition to, expert judgement of wine quality.

Another obvious direction for future research is the linking of the human experts' wine quality ratings with the potential outputs from taste sensors developed in the "AI gustometer" project. This would provide the AI sommelier with an alternative "taste" to learn from.  If the focus remains on human experts' quality ratings, the taste sensor data could instead be used to augment the current training dataset as additional input variables to a model.

# Appendix [for now, general notes]

### Exploration of Data and Transformed Data

```{r explore, eval=FALSE}

# get some summaries of the transformed data
summarytools::dfSummary(wine)
# some data still looks pretty skewed, even after a log transform and standardising

# distribution of wine quality as a factor
summarytools::freq(wine$quality)

# distribution of wine quality by color
with(recode_color(wine), summarytools::ctable(color, quality))

# descriptive statistics for all variables - untransformed
summarytools::descr(origwine, transpose = T)

# descriptive statistics for all variables - transformed
summarytools::descr(wine, transpose = T)

# descriptive statistics by wine color (using untransformed data)
origwine_stats_by_color <- by(data = recode_color(origwine), 
                           INDICES = origwine$color, 
                           FUN = descr, stats = c("mean", "sd", "min", "med", "max"), transpose = TRUE)
view(origwine_stats_by_color, method = "pander")

# descriptive statistics by wine color (using transformed data)
wine_stats_by_color <- by(data = recode_color(wine), 
                           INDICES = wine$color, 
                           FUN = descr, stats = c("mean", "sd", "min", "med", "max"), transpose = TRUE)
view(wine_stats_by_color, method = "pander")
```


## Clustering
We also try clustering and observe  __ 

<cluster graphs>
```{r mclust, eval=FALSE}
## do clusgap (takes a while)
## also look at mclust/hclust
## look at pairs plot
## look at sulfates v quality, clusters v sulfates, quality v clusters
## scale as well
## consider looking at ARI/ASW

mclust_results <-
  select(wine, -quality) %>%
  Mclust(1:15)
saveRDS(mclust_results, './intermediate/mclust_results.rds')
```

```{r plot-clustering, warning=FALSE}
mclust_results <- readRDS('./intermediate/mclust_results.rds')
wine$cluster <- factor(mclust_results$classification)

# do we observe clearly-delineated clusters?
wine %>%
  ggpairs(
      mapping = aes(color = cluster, alpha = 0.4),
      columns = c(variable_var$key[1:n])
  )
  
# do they divide up quality in any meaningful way?
# not especially! perhaps this isn't the most salient feature
wine %>%
  ggplot(aes(x = quality)) +
  geom_bar(aes(fill = cluster))
  
# also examine relationship to color
wine %>%
  recode_color %>%
  ggplot(aes(x = color)) +
  geom_bar(aes(fill = cluster))

# remove cluster variable
wine$cluster <- NULL
```
