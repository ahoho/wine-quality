---
title: "STATG019 ICA2 Part A"
author: "Student Number: 16044460"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r load_pkgs}

# use pacman for package management
if(!require("pacman")) {
    install.packages("pacman")
}

# load the tidyverse, joyplots (now in ggridges) for plots, magrittr for pipes, summarytools for descriptives
pacman::p_load(tidyverse,  ggridges, magrittr, summarytools)
# load the ML libraries needed for benchmarking
pacman::p_load(mlr, kernlab, h2o)

```

```{r unzip}

# what is in the zip file?
unzip(zipfile = "winequality.zip", list = T)
# unzip the directory
unzip(zipfile = "winequality.zip")

```


```{r load_data}

# read in the csv files
red <- read.csv(file = "winequality/winequality-red.csv", header = T, sep = ";")
white <- read.csv(file = "winequality/winequality-white.csv", header = T, sep = ";")

# add a dummy that is 1 for red wine and 0 for white wine (number of obs: red wine - 1599; white wine - 4898)
red$winecolour <- 1
white$winecolour <- 0

# check the columns are the same in both dfs
names(red) == names(white)

# stack the two dataframes to make a single wine df
wine <- rbind(red, white)

# check the result
dim(wine)
str(wine)
head(wine)
summary(wine)

# make the quality variable a factor? - median value is 6
wine$quality <- as.factor(wine$quality)

# make the colour variable a factor
wine$winecolour <- factor(wine$winecolour, labels = c("white", "red"))

# remove the red and white dfs, just use the joint one now
rm(red, white)
```

```{r variable_transformations}

# Do we need to convert the concentrations that are mg/dm3 compared to those that are g/dm3?

# convert all concentration variables to the log scale
# then standardise all numeric vars

origwine <- wine

concvars <- names(wine)[c(1:7, 10)]

wine <- 
    origwine %>% 
    mutate_at(.vars = vars(concvars),
              .funs = log1p) %>% 
    mutate_if(is.numeric, scale)
```


```{r explore}

# get some summaries of the transformed data
view(summarytools::dfSummary(wine))
# some data still looks pretty skewed, even after a log transform and standardising

# distribution of wine quality as a factor
summarytools::freq(wine$quality)

# distribution of wine quality by colour
with(wine, summarytools::ctable(winecolour, quality))

# descriptive statistics for all variables - untransformed
summarytools::descr(origwine, transpose = T)

# descriptive statistics for all variables - transformed
summarytools::descr(wine, transpose = T)

# descriptive statistics by wine colour (using untransformed data)
origwine_stats_by_colour <- by(data = origwine, 
                           INDICES = origwine$winecolour, 
                           FUN = descr, stats = c("mean", "sd", "min", "med", "max"), transpose = TRUE)
view(origwine_stats_by_colour, method = "pander")

# descriptive statistics by wine colour (using transformed data)
wine_stats_by_colour <- by(data = wine, 
                           INDICES = wine$winecolour, 
                           FUN = descr, stats = c("mean", "sd", "min", "med", "max"), transpose = TRUE)
view(wine_stats_by_colour, method = "pander")
```

```{r plots}

# pairs plot - takes a while to plot so commented out
# mypairs <- GGally::ggpairs(data = wine, mapping = aes(colour = winecolour, alpha = 0.4))

# ridge plot function

ridgeplot <- function(winevar) {
    
    origwine %>%
        ggplot(aes(y = quality)) +
        geom_density_ridges(aes_q(x = as.name(winevar), fill = paste(origwine$quality, origwine$winecolour)), 
                            alpha = .8, color = "white", rel_min_height = 0.01) +
        labs(x = Hmisc::capitalize(gsub(x = winevar,
                                            pattern = "\\.",
                                            replacement = " ")),
             y = "Wine quality") +
        scale_y_discrete(expand = c(0.01, 0)) +
        scale_x_continuous(expand = c(0.01, 0)) +
        scale_fill_cyclical(breaks = c("3 white", "3 red"),
                            labels = c(`3 white` = "White", `3 red` = "Red"),
                            values = c("#E31A1C", "#33A02C", "#FB9A99", "#B2DF8A"),
                            name = "Wine colour", guide = "legend") +
        theme_ridges(grid = FALSE, font_size = 11)
}

ridgeplotlist <- lapply(names(origwine)[1:11], ridgeplot)
ridgeplots_grid <- ggpubr::ggarrange(plotlist = ridgeplotlist, ncol = 2, nrow = 2)
# write plots to PDF
if(!file.exists("winevars_ridgeplots.pdf")){
    ggpubr::ggexport(ridgeplots_grid, filename = "winevars_ridgeplots.pdf")
}
```

```{r modelling_regr}

# benchmarking for regression

# 1. make regression learners - 4 types
learners <- 
    makeLearners(cls = c("lm", "h2o.randomForest", "ksvm", "h2o.deeplearning"), type = "regr", ids = c("lm", "rf", "ksvm", "nn"), predict.type = "response")

# 2. make regression task

# turn output variable back into a numeric
wine$quality <- as.numeric(as.character(wine$quality))

# make the task
task <- 
    makeRegrTask(id = "regr_wine", data = wine, target = "quality")

# 3. decide the success measures for regression - MSE and explained variance, along with time for training
measures <- 
    list(mse, expvar, timetrain) 

# 4. tune hyperparameters

# make a model multiplexer to tune all the learners
lrn <-  makeModelMultiplexer(learners)

# set the search space of the tunable parameters for each learner
ps <- 
    makeModelMultiplexerParamSet(
        lrn,
        rf = makeParamSet(
            makeIntegerParam("ntrees", lower = 1L, upper = 500L)),
        ksvm = makeParamSet(
            makeNumericParam("C", lower = 1, upper = 5, trafo = function(x) 2^x),
            makeNumericParam("sigma", lower = 1, upper = 5, trafo = function(x) 2^x)
),
        nn = makeParamSet(
            makeIntegerParam("hidden", lower = 1, upper = 8, trafo = function(x) 2^x))
    )
print(ps)

# set the resampling evaluation method - 3 fold CV
rdesc <- makeResampleDesc("CV", iters = 3L)

# specify the optimisation algorithm    
# to save some time we use random search. but you probably want something like this:
# ctrl = makeTuneControlIrace(maxExperiments = 500L)
ctrl <- makeTuneControlRandom(maxit = 10L)

# perform the tuning use MSE as the tuning measure
res <- tuneParams(learner = lrn, 
                  task = task, 
                  resampling = rdesc, 
                  par.set = ps, 
                  control = ctrl, 
                  measures = list(mse, setAggregation(mse, test.sd)), 
                  show.info = TRUE)

# look at the results
print(res)

# Tune result:
# Op. pars: selected.learner=rf; rf.ntrees=437
# mse.test.mean=0.3750933,mse.test.sd=0.0227736
tune_results_df <- as.data.frame(res$opt.path)

# get the best hyperparameters by learner type
best_tuned <- 
    tune_results_df %>% 
    group_by(selected.learner) %>% 
    filter(mse.test.mean == min(mse.test.mean))

# 5. estimate error out-of-sample for all models and baselines
valsetup <-
    makeResampleDesc("CV", iters = 5)

# 6. run benchmarking experiment (using tuned parameters from above)
regr_bmresults <-
    benchmark(
        learners = list(
            makeLearner(cl = "regr.lm", id = "lm", predict.type = "response"),
            makeLearner(cl = "regr.h2o.randomForest", id = "rf", predict.type = "response", par.vals = list(ntrees = best_tuned[best_tuned$selected.learner == "rf",]$rf.ntrees)),
            makeLearner(cl = "regr.ksvm", id = "ksvm", predict.type = "response", par.vals = list(C = best_tuned[best_tuned$selected.learner == "ksvm",]$ksvm.C, sigma = best_tuned[best_tuned$selected.learner == "ksvm",]$ksvm.sigma)),
            makeLearner(cl = "regr.h2o.deeplearning", id = "nn", predict.type = "response", par.vals = list(hidden = best_tuned[best_tuned$selected.learner == "nn",]$nn.hidden))
        ), 
        tasks = task, 
        resamplings = valsetup, 
        measures = measures)

# 7. look at the benchmarking results
getBMRAggrPerformances(regr_bmresults)
getBMRPerformances(regr_bmresults)
getBMRPredictions(regr_bmresults)

# a few built-in plots
regr_bmplot1 <- plotBMRBoxplots(regr_bmresults, measure = mse)
levels(regr_bmplot1$data$task.id) = c("Univariate regression")
levels(regr_bmplot1$data$learner.id) = c("Linear model", "Random forest", "RBF SVM","Neural network")
ggsave(filename = "regr_bmplot1.pdf", regr_bmplot1)

regr_bmplot2 <- plotBMRBoxplots(regr_bmresults, measure = mse, style = "violin", pretty.names = TRUE) + aes(color = learner.id) + theme(strip.text.x = element_text(size = 8), legend.position = "none")
levels(regr_bmplot2$data$task.id) = c("Univariate regression")
levels(regr_bmplot2$data$learner.id) = c("Linear model", "Random forest", "RBF SVM","Neural network")
ggsave(filename = "regr_bmplot2.pdf", regr_bmplot2)

## DO THE REST OF THE HYPOTHESIS TESTS AND TASKS COMPARISONS AT THE END OF THIS, ONCE WE HAVE CLASSIFICATION BENCHMARK RESULTS TOO https://mlr-org.github.io/mlr-tutorial/release/html/benchmark_experiments/index.html#benchmark-analysis-and-visualization ##

# 8. try to calculate bootstrapped confidence intervals of the test MSEs

get_mse_cis <- function(mse_vector) {
    b <- boot::boot(data = mse_vector, function(u,i) mean(u[i]), R = 999)
    mseci <- boot::boot.ci(b, type = c("norm"))
}

lm_mse_cis <- get_mse_cis(getBMRPerformances(regr_bmresults)$regr_wine$lm$mse)
rf_mse_cis <- get_mse_cis(getBMRPerformances(regr_bmresults)$regr_wine$rf$mse)
ksvm_mse_cis <- get_mse_cis(getBMRPerformances(regr_bmresults)$regr_wine$ksvm$mse)
nn_mse_cis <- get_mse_cis(getBMRPerformances(regr_bmresults)$regr_wine$nn$mse)

# get the regression models' confidence intervals
regr_mse_cis <- list("lm" = c(lm_mse_cis$t0, lm_mse_cis$normal[2:3]), "rf" = c(rf_mse_cis$t0, rf_mse_cis$normal[2:3]), "ksvm" = c(ksvm_mse_cis$t0, ksvm_mse_cis$normal[2:3]), "nn" = c(nn_mse_cis$t0, nn_mse_cis$normal[2:3]))

# NEED TO PUT BENCHMARK RESULTS INTO A TABLE
```

```{r modelling_classif}

# benchmarking for classification

# 1. make classification learners - 4 types
learners = list(
            # makeLearner(cl = "classif.h2o.glm", id = "lm", predict.type = "response", link = "logit"),
            makeLearner(cl = "classif.h2o.randomForest", id = "rf", predict.type = "response"),
            makeLearner(cl = "classif.ksvm", id = "ksvm", predict.type = "response"),
            makeLearner(cl = "classif.h2o.deeplearning", id = "nn", predict.type = "response")
        )

# 2. make classification task

# turn output variable back into a factor
wine$quality <- as.factor(wine$quality)

# make the task
task <- 
    makeClassifTask(id = "classif_wine", data = wine, target = "quality")

# 3. decide the success measures for classification - accuracy and F1 score, along with time for training
measures <- 
    list(acc, timetrain) 

# 4. tune hyperparameters

# make a model multiplexer to tune all the learners
lrn <-  makeModelMultiplexer(learners)

# set the search space of the tunable parameters for each learner
ps <- 
    makeModelMultiplexerParamSet(
        lrn,
        rf = makeParamSet(
            makeIntegerParam("ntrees", lower = 1L, upper = 200L)),
        ksvm = makeParamSet(
            makeNumericParam("C", lower = 1, upper = 5, trafo = function(x) 2^x),
            makeNumericParam("sigma", lower = 1, upper = 5, trafo = function(x) 2^x)
),
        nn = makeParamSet(
            makeIntegerParam("hidden", lower = 1, upper = 8, trafo = function(x) 2^x))
    )
print(ps)

# set the resampling evaluation method - 3 fold CV
rdesc <- makeResampleDesc("CV", iters = 3L)

# specify the optimisation algorithm    
# to save some time we use random search. but you probably want something like this:
# ctrl = makeTuneControlIrace(maxExperiments = 500L)
ctrl <- makeTuneControlRandom(maxit = 10L)

# perform the tuning use accuracy as the tuning measure
res <- tuneParams(learner = lrn, 
                  task = task, 
                  resampling = rdesc, 
                  par.set = ps, 
                  control = ctrl, 
                  measures = list(acc, setAggregation(acc, test.sd)), 
                  show.info = TRUE)

# look at the results
print(res)

# Tune result:
# Op. pars: selected.learner=rf; rf.ntrees=134
# acc.test.mean=0.6720016,acc.test.sd=0.0118452
tune_results_df <- as.data.frame(res$opt.path)

# get the best hyperparameters by learner type
best_tuned <- 
    tune_results_df %>% 
    group_by(selected.learner) %>% 
    filter(acc.test.mean == min(acc.test.mean))

# 5. estimate error out-of-sample for all models and baselines
valsetup <-
    makeResampleDesc("CV", iters = 5)

# 6. run benchmarking experiment (using tuned parameters from above)
classif_bmresults <-
    benchmark(
        learners = list(
            # makeLearner(cl = "classif.h2o.glm", id = "lm", predict.type = "response", link = "logit"),
            makeLearner(cl = "classif.h2o.randomForest", id = "rf", predict.type = "response", par.vals = list(ntrees = best_tuned[best_tuned$selected.learner == "rf",]$rf.ntrees)),
            makeLearner(cl = "classif.ksvm", id = "ksvm", predict.type = "response", par.vals = list(C = best_tuned[best_tuned$selected.learner == "ksvm",]$ksvm.C, sigma = best_tuned[best_tuned$selected.learner == "ksvm",]$ksvm.sigma)),
            makeLearner(cl = "classif.h2o.deeplearning", id = "nn", predict.type = "response", par.vals = list(hidden = best_tuned[best_tuned$selected.learner == "nn",]$nn.hidden))
        ), 
        tasks = task, 
        resamplings = valsetup, 
        measures = measures)

# 7. look at the benchmarking results
getBMRAggrPerformances(classif_bmresults)
getBMRPerformances(classif_bmresults)
getBMRPredictions(classif_bmresults)

# a few built-in plots
classif_bmplot1 <- plotBMRBoxplots(classif_bmresults, measure = acc)
levels(classif_bmplot1$data$task.id) = c("Deterministic classification")
levels(classif_bmplot1$data$learner.id) = c("Random forest", "RBF SVM","Neural network")
classif_bmplot1
ggsave(filename = "classif_bmplot1.pdf", classif_bmplot1)


classif_bmplot2 <- plotBMRBoxplots(classif_bmresults, measure = acc, style = "violin", pretty.names = TRUE) + aes(color = learner.id) + theme(strip.text.x = element_text(size = 8), legend.position = "none")
levels(classif_bmplot2$data$task.id) = c("Deterministic classification")
levels(classif_bmplot2$data$learner.id) = c("Random forest", "RBF SVM","Neural network")
ggsave(filename = "classif_bmplot12.pdf", classif_bmplot2)

## DO THE REST OF THE HYPOTHESIS TESTS AND TASKS COMPARISONS AT THE END OF THIS, ONCE WE HAVE CLASSIFICATION BENCHMARK RESULTS TOO https://mlr-org.github.io/mlr-tutorial/release/html/benchmark_experiments/index.html#benchmark-analysis-and-visualization ##

# 8. try to calculate bootstrapped confidence intervals of the test MSEs

get_mse_cis <- function(mse_vector) {
    b <- boot::boot(data = mse_vector, function(u,i) mean(u[i]), R = 999)
    mseci <- boot::boot.ci(b, type = c("norm"))
}

lm_mse_cis <- get_mse_cis(getBMRPerformances(regr_bmresults)$regr_wine$lm$mse)
rf_mse_cis <- get_mse_cis(getBMRPerformances(regr_bmresults)$regr_wine$rf$mse)
ksvm_mse_cis <- get_mse_cis(getBMRPerformances(regr_bmresults)$regr_wine$ksvm$mse)
nn_mse_cis <- get_mse_cis(getBMRPerformances(regr_bmresults)$regr_wine$nn$mse)

# get the regression models' confidence intervals
classif_mse_cis <- list("lm" = c(lm_mse_cis$t0, lm_mse_cis$normal[2:3]), "rf" = c(rf_mse_cis$t0, rf_mse_cis$normal[2:3]), "ksvm" = c(ksvm_mse_cis$t0, ksvm_mse_cis$normal[2:3]), "nn" = c(nn_mse_cis$t0, nn_mse_cis$normal[2:3]))

# NEED TO PUT BENCHMARK RESULTS INTO A TABLE
```


```{r h2o_shutdown}
# close h2o connection
h2o::h2o.shutdown(prompt = FALSE)
```

