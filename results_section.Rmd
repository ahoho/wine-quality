---
title: "Benchmarking results"
output: bookdown::pdf_document2
header-includes:
  - \usepackage{booktabs}
  - \usepackage{longtable}
  - \usepackage{array}
  - \usepackage{multirow}
  - \usepackage[table]{xcolor}
  - \usepackage{wrapfig}
  - \usepackage{float}
  - \usepackage{colortbl}
  - \usepackage{pdflscape}
  - \usepackage{tabu}
  - \usepackage{threeparttable}
  - \usepackage[normalem]{ulem}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, cache = TRUE, include = FALSE, fig.pos = "H")
```

```{r loadpkgs, include=FALSE}
library(pacman)
p_load(tidyverse, broom, knitr, stringr, here, purrr, kableExtra)
p_load(magrittr)
```


```{r make_benchmark_outputs, include=FALSE}

# Function to make formatted benchmark results table
benchmarktable <- function(file, task, measure) {

df <- read.csv(file = here("output", file))
capvars <- paste(str_split(str_remove(file, ".csv"), "_", simplify = TRUE)[3:2], collapse = " ")

df %>% 
  filter(type == task) %>% 
  filter(metric %in% measure) %>% 
  select(-type) %>%
  mutate(metric = case_when(
    metric == "f1_score" ~ "Accuracy",
    TRUE ~ str_to_upper(metric)
    )) %>% 
  mutate(model = case_when(
    model %in% c("dummy","linear") ~ str_to_title(model),
    TRUE ~ str_to_upper(model)
    )) %>% 
  rename_at(., vars(starts_with("m")), str_to_title) %>% 
  rename_at(., vars(ends_with("b")), str_to_upper) %>% 
  kable(digits = 4,
        booktabs = T,
        format = "latex",
        caption = paste("Benchmarking", "results -", ifelse(task == "reg", "regression", "classification"), "task with", capvars, "variables")) %>% 
  kableExtra::row_spec(0, bold = TRUE) %>% 
  kableExtra::kable_styling(latex_options = "hold_position")

}

# Function to make formatted benchmarking plots

benchmarkplot <- function(file, task, measure) {
  
  df <- read.csv(file = here("output", file))
  capvars <- paste(str_split(str_remove(file, ".csv"), "_", simplify = TRUE)[3:2], collapse = " ")
  fmeasure <- ifelse(measure == "f1_score", "Accuracy", str_to_upper(measure))
  
  df %>% 
    filter(type == task) %>% 
    filter(metric %in% measure) %>% 
    select(-type) %>%
    mutate(metric = case_when(
      metric == "f1_score" ~ str_replace(str_to_title(metric), "_", " "),
      TRUE ~ str_to_upper(metric)
    )) %>% 
    mutate(model = case_when(
      model %in% c("dummy","linear") ~ str_to_title(model),
      TRUE ~ str_to_upper(model)
    )) %>% 
    rename_at(., vars(starts_with("m")), str_to_title) %>% 
    rename_at(., vars(ends_with("b")), str_to_upper) %>% 
    ggplot(aes(Mean, Model, color = Model)) +
    geom_point() +
    geom_errorbarh(aes(xmin = LB, xmax = UB)) +
    theme(legend.position = "none") +
    xlab(paste("Mean", fmeasure)) +
    theme(text = element_text(size = 12)) 
  # +
  #   ggtitle(paste("Benchmarking", fmeasure, "results -", ifelse(task == "reg", "regression", "classification"), "task with", capvars, "variables")) +
  #   theme(plot.title = element_text(size = 10))
}

# a function to get the stat I want
get_stat <- function(file, task, learner, measure, digits = 3) {
  
  df <- read.csv(file = here::here("output", file))

  df %>% 
    filter(type == task) %>% 
    filter(metric == measure) %>% 
    filter(model == learner) %>% 
    select(mean) %>% 
    round(3) %>% 
    as_vector()
  
}

# Get the parameters into lists for pmap

files <- c("results_unscaled_all.csv", "results_chemical_all.csv", "results_color_all.csv")
tasks <- c("reg", "clf")
measures <- c("mse", "mae", "f1_score")

benchdata <- list(files = files, tasks = tasks, measures = measures)

# Make the benchmarking tables
benchtablelist <- 
  benchdata %>%
    purrr::cross_df() %>% 
    filter(!(tasks == "reg" & measures == "f1_score")) %>% 
    pmap(., ~ benchmarktable(..1, ..2, ..3))

# Make the benchmarking plots
benchplotlist <- 
  benchdata %>%
  purrr::cross_df() %>% 
  filter(!(tasks == "reg" & measures == "f1_score")) %>% 
  pmap(., ~ benchmarkplot(..1, ..2, ..3))

```

# Results 

## Results of the predictive benchmarking experiment

### Regression task

The results of the predictive benchmarking experiment for the regression task can be seen in Table \@ref(tab:regtask-results-tab). The random forest algorithm is the best performer on both metrics under consideration for the regression task, achieving a mean squared error of `r get_stat(file = "results_unscaled_all.csv", task = "reg", learner = "rf", measure = "mse")` and a mean absolute error of `r get_stat(file = "results_unscaled_all.csv", task = "reg", learner = "rf", measure = "mae")`. For comparison, the dummy regressor achieved a mean squared error of `r get_stat(file = "results_unscaled_all.csv", task = "reg", learner = "dummy", measure = "mse")` and a mean absolute error of `r get_stat(file = "results_unscaled_all.csv", task = "reg", learner = "dummy", measure = "mae")`. On the MSE metric, the SVM is the next best performer, followed by the linear model, the MLP, and finally the dummy regressor.

```{r regtask-results-tab, echo=FALSE}

benchmarktable(file = "results_unscaled_all.csv", 
               task = "reg", 
               measure = c("mse", "mae"))
```

Figure \@ref(fig:regtask-results-fig) plots these results for each algorithm.

```{r regtask-results-fig, echo=FALSE, fig.cap="Benchmarking results - regression task with all variables", fig.show=, out.width='50%'}

benchmarkplot(file = "results_unscaled_all.csv", 
              task = "reg", 
              measure = "mse") 

benchmarkplot(file = "results_unscaled_all.csv", 
              task = "reg", 
              measure = "mae") 

```


The performance of the algorithms on the same task but with a reduced set of predictor variables can be seen in Figure \@ref(fig:regtask-results-fig2). The left-hand plot shows model performance (mean squared error) with wine colour as the only predictor. The right-hand plot shows results for physiochemical predictors only. Interestingly, all five algorithms have an almost identical, large, MSE (`r mean(sapply(X = c("rf", "linear", "mlp", "svm", "dummy"), function(x) get_stat(file = "results_color_all.csv", task = "reg", learner = x, measure = "mse", digits = 3)))`) for the wine colour only model, suggesting that wine colour alone is not a strong predictor of quality. In contrast, replacing wine colour with the physiochemical characteristics of a wine leads to differential performance across algorithms, and an average MSE of `r mean(sapply(X = c("rf", "linear", "mlp", "svm"), function(x) get_stat(file = "results_chemical_all.csv", task = "reg", learner = x, measure = "mse", digits = 3)))` for all algorithms excluding the dummy regressor, which had an unchanged performance.

```{r regtask-results-fig2, echo=FALSE, fig.cap="Benchmarking results - regression task with wine colour only (left) and physiochemical variables only (right)", fig.show=, out.width='50%'}

benchmarkplot(file = "results_color_all.csv", 
              task = "reg", 
              measure = "mse") 

benchmarkplot(file = "results_chemical_all.csv", 
              task = "reg", 
              measure = "mse") 

```


### Classification task

The results of the predictive benchmarking experiment for the classification task can be seen in Table \@ref(tab:clftask-results-tab). As with the regression task, the random forest model was again the best performer, achieving classification accuracy of `r get_stat(file = "results_unscaled_all.csv", task = "clf", learner = "rf", measure = "f1_score")` and a mean absolute error `r get_stat(file = "results_unscaled_all.csv", task = "clf", learner = "rf", measure = "mae")`. For comparison, the dummy regressor achieved a classification accuracy of `r get_stat(file = "results_unscaled_all.csv", task = "clf", learner = "dummy", measure = "f1_score")` and a mean absolute error `r get_stat(file = "results_unscaled_all.csv", task = "clf", learner = "dummy", measure = "mae")`. The ranking of algorithms by classification accuracy was the same as for the regressio task, with the SVM following the RF, then the linear model, the MLP, and the dummy regressor in fifth position.

```{r clftask-results-tab, echo=FALSE}

benchmarktable(file = "results_unscaled_all.csv", 
               task = "clf", 
               measure = c("f1_score", "mae"))

```


Figure \@ref(fig:clftask-results-fig) plots these results for each algorithm.

```{r clftask-results-fig, echo=FALSE, fig.cap="Benchmarking results - classification task with all variables", fig.show=, out.width='50%'}

benchmarkplot(file = "results_unscaled_all.csv", 
              task = "clf", 
              measure = "f1_score") 

benchmarkplot(file = "results_unscaled_all.csv", 
              task = "clf", 
              measure = "mae") 

```

The performance of the algorithms on the same task but with a reduced set of predictor variables can be seen in Figure \@ref(fig:clftask-results-fig2). The left-hand plot shows model accuracy with wine colour as the only predictor. The right-hand plot shows results for physiochemical predictors only. Interestingly, all algorithms except the dummy regressor achieve identical classification accuracy (`r mean(sapply(X = c("rf", "linear", "mlp", "svm", "dummy"), function(x) get_stat(file = "results_color_all.csv", task = "clf", learner = x, measure = "f1_score", digits = 3)))`) when using wine colour as the only model feature. As with the regression task, replacing wine colour with the physiochemical characteristics of a wine leads to differential performance across algorithms, and an average MSE of `r mean(sapply(X = c("rf", "linear", "mlp", "svm"), function(x) get_stat(file = "results_chemical_all.csv", task = "clf", learner = x, measure = "mse", digits = 3)))` for all algorithms excluding the dummy regressor, which was again unchanged.

```{r clftask-results-fig2, echo=FALSE, fig.cap="Benchmarking results - classification task with wine colour only (left) and physiochemical variables only (right)", fig.show=, out.width='50%'}

benchmarkplot(file = "results_color_all.csv", 
              task = "clf", 
              measure = "f1_score") 

benchmarkplot(file = "results_chemical_all.csv", 
              task = "clf", 
              measure = "f1_score") 

```

## Important components of a "good" wine

A useful characteristic of the random forest algorithm is its ability to determine the relative salience of the various features utilised during training. 

```{r rf-impvar-results-tab, echo=FALSE}

rf_impvar <- read.csv(here("output", "rf_importances_all.csv"))

rf_impvar %>% 
  mutate(feature = as.character(feature)) %>% 
  mutate(feature = case_when(
    feature == "pH" ~ "pH",
    TRUE ~ Hmisc::capitalize(feature)
  )) %>% 
  kable(format = "latex", 
        booktabs = T,
        digits = 3, 
        col.names = c("Feature", "Mean", "SD"), 
        caption = "Relative feature importance in classifying wine quality")

```

```{r rf_impvar_get_stat, include=FALSE}

# a function to get key stats from the RF importance table

rf_get_stat <- function(feat) {
  
  rf_impvar <- read.csv(here::here("output", "rf_importances_all.csv"))

  rf_impvar %>% 
    filter(feature == feat) %>% 
    select(mean) %>% 
    round(3) %>% 
    as_vector()
}

```


Table \@ref(tab:rf-impvar-results-tab) shows the relative importance of each of the variables in the Wine Quality dataset with regards to its use in predicting wine quality. As can be seen, alcohol(vol%) is the most important feature, with a mean importance of `r rf_get_stat("alcohol")`, folowed by volatile acidity i.e. mass concentration of acetic acid (`r rf_get_stat("volatile acidity")`) and density (`r rf_get_stat("density")`). Fixed acidity (`r rf_get_stat("fixed acidity")`) and colour (`r rf_get_stat("color")`) are the least useful features, suggesting wines of both colours and across the range of mass concentrations of tartaric acid can be "good" wines (and "bad" wines).
