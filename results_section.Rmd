---
title: "Benchmarking results"
output: bookdown::pdf_document2
header-includes:
  - \usepackage{booktabs}
  - \usepackage{longtable}
  - \usepackage{array}
  - \usepackage{multirow}
  - \usepackage[table]{xcolor}
  - \usepackage{wrapfig}
  - \usepackage{float}
  - \usepackage{colortbl}
  - \usepackage{pdflscape}
  - \usepackage{tabu}
  - \usepackage{threeparttable}
  - \usepackage[normalem]{ulem}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, cache = TRUE, fig.pos = "H")
```

```{r make_benchmark_outputs, include=FALSE}
# Script to format benchmarking results tables #

library(pacman)
p_load(tidyverse, broom, knitr, stringr, here, purrr, kableExtra)


# Function to make formatted benchmark results table
benchmarktable <- function(file, task, measure, out = "latex") {

df <- read.csv(file = here("output", file))
capvars <- paste(str_split(str_remove(file, ".csv"), "_", simplify = TRUE)[3:2], collapse = " ")

df %>% 
  filter(type == task) %>% 
  filter(metric %in% measure) %>% 
  select(-type,) %>%
  mutate(metric = case_when(
    metric == "f1_score" ~ "Accuracy",
    TRUE ~ str_to_upper(metric)
    )) %>% 
  mutate(model = case_when(
    model %in% c("dummy","linear") ~ str_to_title(model),
    TRUE ~ str_to_upper(model)
    )) %>% 
  rename_at(., vars(starts_with("m")), str_to_title) %>% 
  rename_at(., vars(ends_with("b")), str_to_upper) %>% 
  kable(digits = 4,
        format = out,
        caption = paste("Benchmarking", "results -", ifelse(task == "reg", "regression", "classification"), "task with", capvars, "variables")) %>% 
  kableExtra::row_spec(0, bold = TRUE) %>% 
  kableExtra::kable_styling(latex_options = "hold_position")

}

# Function to make formatted benchmarking plots

benchmarkplot <- function(file, task, measure) {
  
  df <- read.csv(file = here("output", file))
  capvars <- paste(str_split(str_remove(file, ".csv"), "_", simplify = TRUE)[3:2], collapse = " ")
  fmeasure <- ifelse(measure == "f1_score", "Accuracy", str_to_upper(measure))
  
  df %>% 
    filter(type == task) %>% 
    filter(metric %in% measure) %>% 
    select(-type) %>%
    mutate(metric = case_when(
      metric == "f1_score" ~ str_replace(str_to_title(metric), "_", " "),
      TRUE ~ str_to_upper(metric)
    )) %>% 
    mutate(model = case_when(
      model %in% c("dummy","linear") ~ str_to_title(model),
      TRUE ~ str_to_upper(model)
    )) %>% 
    rename_at(., vars(starts_with("m")), str_to_title) %>% 
    rename_at(., vars(ends_with("b")), str_to_upper) %>% 
    ggplot(aes(Mean, Model, color = Model)) +
    geom_point() +
    geom_errorbarh(aes(xmin = LB, xmax = UB)) +
    theme(legend.position = "none") +
    xlab(paste("Mean", fmeasure)) +
    theme(text = element_text(size = 12)) 
  # +
  #   ggtitle(paste("Benchmarking", fmeasure, "results -", ifelse(task == "reg", "regression", "classification"), "task with", capvars, "variables")) +
  #   theme(plot.title = element_text(size = 10))
}

# Get the parameters into lists for pmap

files <- c("results_unscaled_all.csv", "results_chemical_all.csv", "results_color_all.csv")
tasks <- c("reg", "clf")
measures <- c("mse", "mae", "f1_score")

benchdata <- list(files = files, tasks = tasks, measures = measures)

# Make the benchmarking tables
benchtablelist <- 
  benchdata %>%
    purrr::cross_df() %>% 
    filter(!(tasks == "reg" & measures == "f1_score")) %>% 
    pmap(., ~ benchmarktable(..1, ..2, ..3))

# Make the benchmarking plots
benchplotlist <- 
  benchdata %>%
  purrr::cross_df() %>% 
  filter(!(tasks == "reg" & measures == "f1_score")) %>% 
  pmap(., ~ benchmarkplot(..1, ..2, ..3))

```

# Results of predictive benchmarking experiment

## Regression task

The results of the predictive benchmarking experiment for the regression task can be seen in Table \@ref(tab:regtask-results-tab).

```{r regtask-results-tab}

benchmarktable(file = "results_unscaled_all.csv", 
               task = "reg", 
               measure = c("mse", "mae"), out = "latex")

```

Figure \@ref(fig:regtask-results-fig) plots these results for each algorithm on the regression task.

```{r regtask-results-fig, fig.cap="Benchmarking results - regression task with all unscaled variables", out.width = '50%', fig.show = 'hold'}

benchmarkplot(file = "results_unscaled_all.csv", 
              task = "reg", 
              measure = "mse") 

benchmarkplot(file = "results_unscaled_all.csv", 
              task = "reg", 
              measure = "mae") 

```



## Classification task

The results of the predictive benchmarking experiment for the classification task can be seen in Table \@ref(tab:clftask-results-tab).

```{r clftask-results-tab}

benchmarktable(file = "results_unscaled_all.csv", 
               task = "clf", 
               measure = c("f1_score", "mae"), out = "latex")

```


Figure \@ref(fig:clftask-results-fig) plots these results for each algorithm on the regression task.

```{r clftask-results-fig, fig.cap="Benchmarking results - classification task with all unscaled variables", out.width = '50%', fig.show = 'hold'}

benchmarkplot(file = "results_unscaled_all.csv", 
              task = "clf", 
              measure = "f1_score") 

benchmarkplot(file = "results_unscaled_all.csv", 
              task = "clf", 
              measure = "mae") 

```