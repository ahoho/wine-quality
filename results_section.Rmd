---
title: "Benchmarking results"
output: bookdown::pdf_document2
header-includes:
  - \usepackage{booktabs}
  - \usepackage{longtable}
  - \usepackage{array}
  - \usepackage{multirow}
  - \usepackage[table]{xcolor}
  - \usepackage{wrapfig}
  - \usepackage{float}
  - \usepackage{colortbl}
  - \usepackage{pdflscape}
  - \usepackage{tabu}
  - \usepackage{threeparttable}
  - \usepackage[normalem]{ulem}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, cache = TRUE, fig.pos = "H")
```

```{r make_benchmark_outputs, include=FALSE}
# Script to format benchmarking results tables #

library(pacman)
p_load(tidyverse, magrittr, broom, knitr, stringr, here, purrr, kableExtra)

# Function to make formatted benchmark results table
benchmarktable <- function(file, task, measure) {

df <- read.csv(file = here("output", file))
capvars <- paste(str_split(str_remove(file, ".csv"), "_", simplify = TRUE)[3:2], collapse = " ")

df %>% 
  filter(type == task) %>% 
  filter(metric %in% measure) %>% 
  select(-type) %>%
  mutate(metric = case_when(
    metric == "f1_score" ~ "Accuracy",
    TRUE ~ str_to_upper(metric)
    )) %>% 
  mutate(model = case_when(
    model %in% c("dummy","linear") ~ str_to_title(model),
    TRUE ~ str_to_upper(model)
    )) %>% 
  rename_at(., vars(starts_with("m")), str_to_title) %>% 
  rename_at(., vars(ends_with("b")), str_to_upper) %>% 
  kable(digits = 4,
        booktabs = T,
        format = "latex",
        caption = paste("Benchmarking", "results -", ifelse(task == "reg", "regression", "classification"), "task with", capvars, "variables")) %>% 
  kableExtra::row_spec(0, bold = TRUE) %>% 
  kableExtra::kable_styling(latex_options = "hold_position")

}

# Function to make formatted benchmarking plots

benchmarkplot <- function(file, task, measure) {
  
  df <- read.csv(file = here("output", file))
  capvars <- paste(str_split(str_remove(file, ".csv"), "_", simplify = TRUE)[3:2], collapse = " ")
  fmeasure <- ifelse(measure == "f1_score", "Accuracy", str_to_upper(measure))
  
  df %>% 
    filter(type == task) %>% 
    filter(metric %in% measure) %>% 
    select(-type) %>%
    mutate(metric = case_when(
      metric == "f1_score" ~ str_replace(str_to_title(metric), "_", " "),
      TRUE ~ str_to_upper(metric)
    )) %>% 
    mutate(model = case_when(
      model %in% c("dummy","linear") ~ str_to_title(model),
      TRUE ~ str_to_upper(model)
    )) %>% 
    rename_at(., vars(starts_with("m")), str_to_title) %>% 
    rename_at(., vars(ends_with("b")), str_to_upper) %>% 
    ggplot(aes(Mean, Model, color = Model)) +
    geom_point() +
    geom_errorbarh(aes(xmin = LB, xmax = UB)) +
    theme(legend.position = "none") +
    xlab(paste("Mean", fmeasure)) +
    theme(text = element_text(size = 12)) 
  # +
  #   ggtitle(paste("Benchmarking", fmeasure, "results -", ifelse(task == "reg", "regression", "classification"), "task with", capvars, "variables")) +
  #   theme(plot.title = element_text(size = 10))
}

# Get the parameters into lists for pmap

files <- c("results_unscaled_all.csv", "results_chemical_all.csv", "results_color_all.csv")
tasks <- c("reg", "clf")
measures <- c("mse", "mae", "f1_score")

benchdata <- list(files = files, tasks = tasks, measures = measures)

# Make the benchmarking tables
benchtablelist <- 
  benchdata %>%
    purrr::cross_df() %>% 
    filter(!(tasks == "reg" & measures == "f1_score")) %>% 
    pmap(., ~ benchmarktable(..1, ..2, ..3))

# Make the benchmarking plots
benchplotlist <- 
  benchdata %>%
  purrr::cross_df() %>% 
  filter(!(tasks == "reg" & measures == "f1_score")) %>% 
  pmap(., ~ benchmarkplot(..1, ..2, ..3))

```

# Results 

## Results of the predictive benchmarking experiment

### Regression task

The results of the predictive benchmarking experiment for the regression task can be seen in Table \@ref(tab:regtask-results-tab).

```{r regtask-results-tab}

benchmarktable(file = "results_unscaled_all.csv", 
               task = "reg", 
               measure = c("mse", "mae"))

```

Figure \@ref(fig:regtask-results-fig) plots these results for each algorithm.

```{r regtask-results-fig, fig.cap="Benchmarking results - regression task with all variables", out.width = '50%', fig.show = 'hold'}

benchmarkplot(file = "results_unscaled_all.csv", 
              task = "reg", 
              measure = "mse") 

benchmarkplot(file = "results_unscaled_all.csv", 
              task = "reg", 
              measure = "mae") 

```


The performance of the algorithms on the same task but with a reduced set of predictor variables can be seen in Figure \@ref(fig:regtask-results-fig2). The left-hand plot shows model performance (mean squared error) with wine colour as the only predictor. The right-hand plot shows results for physiochemical predictors only.

```{r regtask-results-fig2, fig.cap="Benchmarking results - regression task with wine colour only (left) and physiochemical variables only (right)", out.width = '50%', fig.show = 'hold'}

benchmarkplot(file = "results_color_all.csv", 
              task = "reg", 
              measure = "mse") 

benchmarkplot(file = "results_chemical_all.csv", 
              task = "reg", 
              measure = "mse") 

```


### Classification task

The results of the predictive benchmarking experiment for the classification task can be seen in Table \@ref(tab:clftask-results-tab).

```{r clftask-results-tab}

benchmarktable(file = "results_unscaled_all.csv", 
               task = "clf", 
               measure = c("f1_score", "mae"))

```


Figure \@ref(fig:clftask-results-fig) plots these results for each algorithm.

```{r clftask-results-fig, fig.cap="Benchmarking results - classification task with all variables", out.width = '50%', fig.show = 'hold'}

benchmarkplot(file = "results_unscaled_all.csv", 
              task = "clf", 
              measure = "f1_score") 

benchmarkplot(file = "results_unscaled_all.csv", 
              task = "clf", 
              measure = "mae") 

```

The performance of the algorithms on the same task but with a reduced set of predictor variables can be seen in Figure \@ref(fig:clftask-results-fig2). The left-hand plot shows model accuracy with wine colour as the only predictor. The right-hand plot shows results for physiochemical predictors only.

```{r clftask-results-fig2, fig.cap="Benchmarking results - classification task with wine colour only (left) and physiochemical variables only (right)", out.width = '50%', fig.show = 'hold'}

benchmarkplot(file = "results_color_all.csv", 
              task = "clf", 
              measure = "f1_score") 

benchmarkplot(file = "results_chemical_all.csv", 
              task = "clf", 
              measure = "f1_score") 

```

### Important components of a "good" wine

A useful characteristic of the random forest algorithm is its ability to determine the relative salience of the various features utilised during training. 

```{r rf-impvar-results-tab}

rf_impvar <- read.csv(here("output", "rf_importances_all.csv"))

rf_impvar %>% 
  mutate(feature = as.character(feature)) %>% 
  mutate(feature = case_when(
    feature == "pH" ~ "pH",
    TRUE ~ Hmisc::capitalize(feature)
  )) %>% 
  kable(format = "latex", 
        booktabs = T,
        digits = 3, 
        col.names = c("Feature", "Mean", "SD"), 
        caption = "Relative feature importance in classifying wine quality")

```


Table \@ref(tab:rf-impvar-results-tab) shows the relative importance of each of the variables in the Wine Quality dataset with regards to its use in predicting wine quality. As can be seen, alcohol(vol%) is the most important feature, with a mean importance of `r round(rf_impvar[rf_impvar$feature == "alcohol", 2], 3)`, folowed by volatile acidity i.e. mass concentration of acetic acid (`r round(rf_impvar[rf_impvar$feature == "volatile acidity", 2], 3)` g/dm$^3$) and density (`r round(rf_impvar[rf_impvar$feature == "density", 2], 3)` g/cm$^3$). Fixed acidity (`r round(rf_impvar[rf_impvar$feature == "fixed acidity", 2], 3)` g/cm$^3$) and colour (`r round(rf_impvar[rf_impvar$feature == "color", 2], 3)` g/cm$^3$) are the least useful features, suggesting wines of both colours and across the range of mass concentrations of tartaric acid can be "good" wines (and "bad" wines).




